# -*- coding: utf-8 -*-
"""Brainlink.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WB_lpfLgga0yKCoE47n6kaaoe0pfTrr
"""

#from google.colab import drive
#drive.mount('/content/drive')

#!pip install numpy
#!pip install pandas
import sys
import itertools
import os 
import scipy
import numpy
import wavelet_entropy as waen
from scipy import signal 
import pandas as pd
import matplotlib.pyplot as plt

"""A Norch Filter:
  Band-stop filter
  stop frequency: 50Hz
"""

sample_rate=512


def Notch_filter(sig_data):

  fr=50
  fs=sample_rate
  Q=10
  b,a = signal.iirnotch(fr,Q,fs)
  filted_data=scipy.signal.filtfilt(b,a,sig_data)
   
  b, a = signal.butter(8, [0.004,0.86], 'bandpass')   #配置滤波器 8 表示滤波器的阶数
  filtedData = signal.filtfilt(b, a, filted_data)  #data为要过滤的信号

  return filtedData

"""FFT with hamming window"""

def FFT_ham(sig_data):

  N=5*sample_rate
  
  hamming_win=signal.hamming(N)
  sig_win=sig_data*hamming_win

  sig_fft=numpy.fft.fft(sig_win)

  f=numpy.fft.fftfreq(N, 1/sample_rate)

  
  #plt.plot(f, np.abs(sig_fft))
  #plt.xlabel('Freq')
  #plt.ylabel('Magnitude')
  #plt.show()
  return sig_fft, f

"""# Read txt datafiles: 
* cut the data into 5s/peice. The data is totally aroundd 14min long. 
* Consisit of 1min relax, 4min open eyes, 1min relax, 4min close eyes, 1min relax, 4min open eyes.
* use the signal between 2min-4min and 6.5min-8.5min

"""

FATIGUE = [
   # "FAT_WAK_data/umindsleep/UmindSleep_04_FAT.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_06_FAT.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_08_FAT.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_09_FAT.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_11_FAT.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_12_FAT.txt"
   # "/content/drive/MyDrive/BrainLink/BrainLink2/BrainLinkRaw_02_FAT.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_01_FAT.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_02_FAT.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_03_FAT.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_04_FAT.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_05_FAT.txt"
  
]

WAKE = [
   # "FAT_WAK_data/umindsleep/UmindSleep_04_WAK.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_06_WAK.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_08_WAK.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_09_WAK.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_11_WAK.txt",
   # "FAT_WAK_data/umindsleep/UmindSleep_12_WAK.txt"
       # "/content/drive/MyDrive/BrainLink/BrainLink2/BrainLinkRaw_05_WAK.txt"
   "FAT_WAK_data/brainlink/BrainLinkRaw_01_WAK.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_02_WAK.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_03_WAK.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_04_WAK.txt",
   "FAT_WAK_data/brainlink/BrainLinkRaw_05_WAK.txt"
]

KSS=[]
index = 0
delta, theta, alpha, beta, gamma, fi, KSS = [], [], [], [], [], [], []
SE,RE2,RE3,TsE,GenTsE= [],[],[],[],[]
filtered_data=numpy.zeros(sample_rate*5)
for file in FATIGUE:
    filtered_data=numpy.zeros(sample_rate*5)
    EEG = numpy.loadtxt(file)
    for i in range(24,48):
      filtered_data=Notch_filter((EEG[i*sample_rate*5:(i+1)*sample_rate*5]))
      se,re2,re3,tse,gentse=waen.WE(filtered_data)
      SE.append(se)
      RE2.append(re2)
      RE3.append(re3)
      TsE.append(tse)
      GenTsE.append(gentse)
      sig_fft,freq_fft=FFT_ham(filtered_data)
      eeg_power=numpy.abs(sig_fft)**2
      eeg=eeg_power[:int(sample_rate*5/2)]
      delta.append(numpy.mean(eeg[int(0.5/128*(sample_rate*5/2)):int(4/128*(sample_rate*5/2))]))
      theta.append(numpy.mean(eeg[int(4/128*(sample_rate*5/2)):int(8/128*(sample_rate*5/2))]))
      alpha.append(numpy.mean(eeg[int(7.5/128*(sample_rate*5/2)):int(13/128*(sample_rate*5/2))]))
      beta.append(numpy.mean(eeg[int(13/128*(sample_rate*5/2)):int(30/128*(sample_rate*5/2))]))
      gamma.append(numpy.mean(eeg[int(30/128*(sample_rate*5/2)):int(44/128*(sample_rate*5/2))]))
      fi.append(numpy.mean(eeg[int(0.85/128*(sample_rate*5/2)):int(110/128*(sample_rate*5/2))]))
      
      # KSS.append(KSS_val)
      KSS.append(0)
fkaverageopen=filtered_data/4/24





filtered_data=numpy.zeros(sample_rate*5)
for file in WAKE:
    filtered_data=numpy.zeros(sample_rate*5)
    EEG = numpy.loadtxt(file)
    for i in range(24,48):
      filtered_data=Notch_filter((EEG[i*sample_rate*5:(i+1)*sample_rate*5])) 
      se,re2,re3,tse,gentse=waen.WE(filtered_data)
      SE.append(se)
      RE2.append(re2)
      RE3.append(re3)
      TsE.append(tse)
      GenTsE.append(gentse)     

      sig_fft,freq_fft=FFT_ham(filtered_data)
      eeg_power=numpy.abs(sig_fft)**2
      eeg=eeg_power[:int(sample_rate*5/2)]
      delta.append(numpy.mean(eeg[int(0.5/128*(sample_rate*5/2)):int(4/128*(sample_rate*5/2))]))
      theta.append(numpy.mean(eeg[int(4/128*(sample_rate*5/2)):int(8/128*(sample_rate*5/2))]))
      alpha.append(numpy.mean(eeg[int(7.5/128*(sample_rate*5/2)):int(13/128*(sample_rate*5/2))]))
      beta.append(numpy.mean(eeg[int(13/128*(sample_rate*5/2)):int(30/128*(sample_rate*5/2))]))
      gamma.append(numpy.mean(eeg[int(30/128*(sample_rate*5/2)):int(44/128*(sample_rate*5/2))]))
      fi.append(numpy.mean(eeg[int(0.85/128*(sample_rate*5/2)):int(110/128*(sample_rate*5/2))]))
      KSS.append(1)
   # for i in range(132,157):
  #    filtered_data=filtered_data+Notch_filter((EEG[i*512*5:(i+1)*512*5]))
wkaverageopen=filtered_data/4/24


#print(SE)
#print(RE2)
#print(RE3)
#print(TsE)
#print(GenTsE)

#exit()

filtered_data=numpy.zeros(sample_rate*5)
for file in FATIGUE:
    filtered_data=numpy.zeros(sample_rate*5)
    EEG = numpy.loadtxt(file)
    for i in range(78,102):
      filtered_data=Notch_filter((EEG[i*sample_rate*5:(i+1)*sample_rate*5]))  
      se,re2,re3,tse,gentse=waen.WE(filtered_data)
      SE.append(se)
      RE2.append(re2)
      RE3.append(re3)
      TsE.append(tse)
      GenTsE.append(gentse)     

      sig_fft,freq_fft=FFT_ham(filtered_data)
      eeg_power=numpy.abs(sig_fft)**2
      eeg=eeg_power[:int(sample_rate*5/2)]
      delta.append(numpy.mean(eeg[int(0.5/128*(sample_rate*5/2)):int(4/128*(sample_rate*5/2))]))
      theta.append(numpy.mean(eeg[int(4/128*(sample_rate*5/2)):int(8/128*(sample_rate*5/2))]))
      alpha.append(numpy.mean(eeg[int(7.5/128*(sample_rate*5/2)):int(13/128*(sample_rate*5/2))]))
      beta.append(numpy.mean(eeg[int(13/128*(sample_rate*5/2)):int(30/128*(sample_rate*5/2))]))
      gamma.append(numpy.mean(eeg[int(30/128*(sample_rate*5/2)):int(44/128*(sample_rate*5/2))]))
      fi.append(numpy.mean(eeg[int(0.85/128*(sample_rate*5/2)):int(110/128*(sample_rate*5/2))]))
      KSS.append(0)
fkaverageclose=filtered_data/4/24



filtered_data=numpy.zeros(sample_rate*5)
for file in WAKE:
    filtered_data=numpy.zeros(sample_rate*5)
    EEG = numpy.loadtxt(file)
    for i in range(78,102):
      filtered_data=Notch_filter((EEG[i*sample_rate*5:(i+1)*sample_rate*5])) 
      se,re2,re3,tse,gentse=waen.WE(filtered_data)
      SE.append(se)
      RE2.append(re2)
      RE3.append(re3)
      TsE.append(tse)
      GenTsE.append(gentse)           

      sig_fft,freq_fft=FFT_ham(filtered_data)
      eeg_power=numpy.abs(sig_fft)**2
      eeg=eeg_power[:int(sample_rate*5/2)]
      delta.append(numpy.mean(eeg[int(0.5/128*(sample_rate*5/2)):int(4/128*(sample_rate*5/2))]))
      theta.append(numpy.mean(eeg[int(4/128*(sample_rate*5/2)):int(8/128*(sample_rate*5/2))]))
      alpha.append(numpy.mean(eeg[int(7.5/128*(sample_rate*5/2)):int(13/128*(sample_rate*5/2))]))
      beta.append(numpy.mean(eeg[int(13/128*(sample_rate*5/2)):int(30/128*(sample_rate*5/2))]))
      gamma.append(numpy.mean(eeg[int(30/128*(sample_rate*5/2)):int(44/128*(sample_rate*5/2))]))
      fi.append(numpy.mean(eeg[int(0.85/128*(sample_rate*5/2)):int(110/128*(sample_rate*5/2))]))
      KSS.append(1)
wkaverageclose=filtered_data/4/24

"""# Calculate Sp Features
* 15 features are calculated
* $\delta, \theta, \alpha, \frac{\theta}{\beta}, \frac{\theta}{\alpha}, \frac{\theta}{\phi}, \frac{\theta}{\alpha+\beta+\gamma}, \frac{\delta}{\alpha+\beta+\gamma}, \frac{\delta}{\alpha}, \frac{\delta}{\phi}, \frac{\delta}{\beta}, \frac{\delta}{\theta}, \frac{\theta}{\alpha+\beta+\theta}, \frac{\alpha}{\theta+\alpha+\beta}, \frac{\beta}{\theta+\alpha+\beta}$
"""
#print(SE)
#print(RE2)
#print(RE3)
#print(TsE)
#print(GenTsE)

delta = numpy.array(delta)  
theta = numpy.array(theta)    
alpha = numpy.array(alpha)    
beta = numpy.array(beta)    
gamma = numpy.array(gamma)    
fi = numpy.array(fi)    
 

Sp = [delta, theta, alpha, theta/beta, theta/alpha, theta/fi, theta/alpha+beta+gamma, delta/alpha+beta+gamma, delta/alpha, delta/fi, \
      delta/beta, delta/theta, theta/alpha+beta+theta, alpha/theta+alpha+beta, beta/theta+alpha+beta]

Entropy=[SE,RE2,TsE,GenTsE]

"""# SVM Classify
* classify to fatigue and wake
* shape of KSS and Sp is (384,15), the former 192 points are open eyes data, the later 192 points are close eyes data
"""






#x = numpy.transpose(numpy.array(Entropy))
x = numpy.transpose(numpy.array(Sp))
y = numpy.transpose(numpy.array(KSS))

print(x.shape)
print(y.shape)
#x=x[240:480]
#y=y[240:480]
#x=x[:240]
#=y[:240]
# -*- coding: utf-8 -*-
"""
Created on Wed Aug 10 15:10:37 2022
@author: augustqi
"""
 
"""
版本信息：
python==3.9
numpy==1.22.3
pytorch==1.11.0+cu113
scikit-learn==1.0.2
"""
 
 
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_classification
 
#X,y = make_classification(n_samples=3000, n_features=50, n_informative=40, n_redundant=10, random_state=0)
 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
print(x_train.shape)  
print(x_test.shape)   
print(y_train.shape)  
print(y_test.shape)  
 
 
# # scale data
# # t = MinMaxScaler()
# t = StandardScaler()
# t.fit(x_train)
# x_train = t.transform(x_train)
# x_test = t.transform(x_test)
 
 
x_train_ = torch.Tensor(x_train)
# y_train_ = torch.Tensor(y_train)
y_train_ = torch.LongTensor(y_train)
dataset_train = torch.utils.data.TensorDataset(x_train_, y_train_)
data_iter_train = torch.utils.data.DataLoader(dataset_train, batch_size=30, shuffle=True)
 
x_test_ = torch.Tensor(x_test)
x_test_1 = x_test_.unsqueeze(1)  # 扩充维度
 
 
# 网络
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
 
        self.code = nn.Sequential(
            # 输出通道数==卷积核个数, 卷积->BN->激活->池化
            # (n-k)/s+1
            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=2, stride=1,padding=1),  # (1,15)--> (4,16)
            nn.BatchNorm1d(8),
            nn.ReLU(True),
          #  nn.ReLU(),
            # (n-k)/s+1
            nn.MaxPool1d(kernel_size=2, stride=1,padding=1),  # (4,16)-->(4,15)
 
            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=2, stride=1,padding=1),  # (4,15)-->(8,16)
            nn.BatchNorm1d(16),
            nn.ReLU(True),
          #  nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=1,padding=1),  # (8,16)-->(8,15)
 
            # 展平
            nn.Flatten(),
            nn.Linear(in_features=16*19, out_features=152),
            nn.ReLU(True),
            #nn.ReLU(),
            #nn.Sigmoid(),
            nn.Linear(in_features=152, out_features=76),
            nn.ReLU(True),
            #nn.ReLU(),
            #nn.Sigmoid(),
            nn.Linear(in_features=76, out_features=38),
            nn.ReLU(True),
            nn.Linear(in_features=38, out_features=2), # 二分类
        )
 
    def forward(self, x):
        x = self.code(x)
 
        return x
 
 
# 模型实例化
net = ConvNet()
# 损失函数
criterion = nn.CrossEntropyLoss()
# 优化器
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
 
epochs = 2000
loss_train = []
for epoch in range(epochs):
    # 训练模型 / train
    net.train()
    running_loss = 0.0
    for batch_idx, data in enumerate(data_iter_train):
        input_data = data[0]
      #  print("1",input_data.shape)
        input_data = input_data.unsqueeze(1)  # 扩充维度
      #  print("2",input_data.shape)
        label = data[1]
        # label = label.unsqueeze(1)  # 扩充维度
        # print("label shape",label.shape)
        output_data = net(input_data)
        # output_data_ = torch.argmax(output_data,1)
        # print(output_data)
        # print(output_data.shape)
        loss = criterion(output_data, label)
        # print("loss:",loss)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    # 一个epoch中，每个batchsize的平均lossa
    print("epoch: %d"%epoch, "loss: %f"%(running_loss / len(data_iter_train)))
    loss_train.append(running_loss / len(data_iter_train))
 
 
plt.figure(1)
plt.title('train loss')  # 图片名称
plt.xlabel("epoch")  # 横坐标名称
plt.ylabel("loss")  # 纵坐标名称
plt.plot(np.arange(len(loss_train)), loss_train, label='train')
plt.legend(loc='upper right')
plt.savefig('loss.png', dpi=600)  # 保存图片
plt.show()
 
output_test = net(x_test_1)
output_test_ = torch.argmax(output_test,1)
y_predict = output_test_.detach().numpy().reshape(-1)
acc_test = [((np.sum(y_predict == y_test)) / x_test_.shape[0]) * 100]
print("acc test:",acc_test)